{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Face Swapping\n",
    "\n",
    "This notebook demonstrates the ability to swap faces using numpy, dlib and OpenCV. (Original article on [reddit](https://www.reddit.com/r/programming/comments/3f591x/so_i_wrote_a_script_that_swaps_peoples_faces_in/) by Matthew Earl)\n",
    "\n",
    "Note: The sample images are downloaded from wikimedia.org (see the attached license file). Code and notebook are MIT License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Detecting facial landmarks using _dlib_\n",
    "\n",
    "The first thing we want to do is to extract the landmark matrices, especially the coordinates of the particular facial features.\n",
    "\n",
    "Ref.: [One Millisecond Face Alignment with an Ensemble of Regression Trees](http://www.csc.kth.se/~vahidk/papers/KazemiCVPR14.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the original sample images:\n",
    "<table><tr><td><img src=\"./Andrea_V.jpg\" alt=\"sample 1\" style=\"height: 320px;\"/></td>\n",
    "<td><img src=\"./The_Equestrian_Session.jpg\" alt=\"sample 2\" style=\"height: 320px;\"/></td></tr></table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'cv2'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-c678ec9f455e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# imports\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m#==========\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdlib\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'cv2'"
     ]
    }
   ],
   "source": [
    "#==========\n",
    "# imports\n",
    "#==========\n",
    "import cv2\n",
    "import dlib\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "\n",
    "#=================================\n",
    "# constants and initial settings\n",
    "#=================================\n",
    "sample1 = 'Andrea_V.jpg'\n",
    "sample2 = 'The_Equestrian_Session.jpg'\n",
    "predictor_path = 'shape_predictor_68_face_landmarks.dat'\n",
    "scale_factor = 1\n",
    "feather_amount = 11\n",
    "\n",
    "face_points = list(range(17, 68))\n",
    "mouth_points = list(range(48, 61))\n",
    "right_brow_points = list(range(17, 22))\n",
    "left_brow_points = list(range(22, 27))\n",
    "right_eye_points = list(range(36, 42))\n",
    "left_eye_points = list(range(42, 48))\n",
    "nose_points = list(range(27, 35))\n",
    "jaw_points = list(range(0, 17))\n",
    "\n",
    "# Points used to line up the images.\n",
    "align_points = (\n",
    "    left_brow_points + right_eye_points + left_eye_points + \n",
    "    right_brow_points + nose_points + mouth_points\n",
    ")\n",
    "\n",
    "# Points from the second image to overlay on the first. \n",
    "# The convex hull of each element will be overlaid.\n",
    "overlay_points = [\n",
    "    left_eye_points + right_eye_points + left_brow_points + right_brow_points,\n",
    "    nose_points + mouth_points,\n",
    "]\n",
    "\n",
    "# Amount of blur to use during colour correction, as a fraction of the\n",
    "# pupillary distance.\n",
    "color_correction_blur_fraction = 0.6\n",
    "\n",
    "#===============================\n",
    "# helper classes and functions\n",
    "#===============================\n",
    "class TooManyFacesException(Exception):\n",
    "    pass\n",
    "\n",
    "class NoFacesException(Exception):\n",
    "    pass\n",
    "\n",
    "def read_image(img_path):\n",
    "    \"\"\"\n",
    "    read and resize raw color image to numpy array\n",
    "    \"\"\"\n",
    "    img = cv2.imread(img_path, cv2.IMREAD_COLOR)\n",
    "    img = cv2.resize(img, (\n",
    "        img.shape[1] * scale_factor, \n",
    "        img.shape[0] * scale_factor\n",
    "    ))\n",
    "    return img\n",
    "\n",
    "def write_image(img_name, img):\n",
    "    \"\"\"\n",
    "    writes image into the generated folder\n",
    "    \"\"\"\n",
    "    generated_pth = 'generated/{}'.format(img_name)\n",
    "    cv2.imwrite(generated_pth, img)\n",
    "\n",
    "if not os.path.exists('./generated'):\n",
    "    os.mkdir('./generated')\n",
    "\n",
    "# download pretrained dlib predictor dat (97mb)\n",
    "import urllib.request\n",
    "if not os.path.exists(predictor_path):\n",
    "    urllib.request.urlretrieve (\n",
    "        'https://github.com/AKSHAYUBHAT/TensorFace/raw/master/openface/models/dlib/{}'.format(\n",
    "            predictor_path\n",
    "        ), \n",
    "        predictor_path\n",
    "    )\n",
    "    \n",
    "print('> ready!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detector = dlib.get_frontal_face_detector()\n",
    "# dlib feature extractor with a pre-trained model\n",
    "predictor = dlib.shape_predictor(predictor_path)\n",
    "\n",
    "def get_landmarks(img):\n",
    "    \"\"\"\n",
    "    takes an image in form of a numpy array and returns a 68x2 matrix, \n",
    "    each row corresponds with the x/y-coordinates of a particular \n",
    "    feature point in the input image.\n",
    "\n",
    "    A rough bounding box for the feature extractor (predictor)\n",
    "    provided by a traditional face detector (detector) which returns\n",
    "    a list of rectangles, each of which corresponds to a face in\n",
    "    the image\n",
    "    \"\"\"\n",
    "    rects = detector(img, 1)\n",
    "    if len(rects)>1:\n",
    "        raise TooManyFacesException\n",
    "    if len(rects)==0:\n",
    "        raise NoFacesException\n",
    "    return np.matrix(\n",
    "        [[p.x, p.y] for p in predictor(img, rects[0]).parts()]\n",
    "    )\n",
    "\n",
    "def annotate_landmarks(img, landmarks):\n",
    "    \"\"\"\n",
    "    annotates feature landmarks on the image.\n",
    "    \"\"\"\n",
    "    img = img.copy()\n",
    "    for idx, point in enumerate(landmarks):\n",
    "        pos = (point[0, 0], point[0, 1])\n",
    "        cv2.putText(img, str(idx), pos,\n",
    "                    fontFace=cv2.FONT_HERSHEY_SCRIPT_SIMPLEX,\n",
    "                    fontScale=0.4,\n",
    "                    color=(0, 0, 255))\n",
    "        cv2.circle(img, pos, 3, color=(0, 255, 255))\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate annotated images for debugging purposes\n",
    "data = {}\n",
    "for pth in [sample1, sample2]:\n",
    "    data[pth] = {}\n",
    "    data[pth]['raw_image'] = read_image(pth)\n",
    "    data[pth]['landmarks'] = get_landmarks(data[pth]['raw_image'])\n",
    "    data[pth]['annotated_image'] = annotate_landmarks(\n",
    "        data[pth]['raw_image'], \n",
    "        data[pth]['landmarks']\n",
    "    )\n",
    "    write_image(\n",
    "        'annotated_{}'.format(pth), \n",
    "        data[pth]['annotated_image']\n",
    "    )\n",
    "    print('> generated annotated image of \\'{}\\''.format(pth))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Annotated sample images with landmarks:\n",
    "<table><tr><td><img src=\"./generated/annotated_Andrea_V.jpg\" alt=\"annotated sample 1\" style=\"height: 320px;\"/></td>\n",
    "<td><img src=\"./generated/annotated_The_Equestrian_Session.jpg\" alt=\"annotated sample 2\" style=\"height: 320px;\"/></td></tr></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Align faces with a procrustes analysis\n",
    "\n",
    "Now we're going to rotate, scale and translate the feature points of the image vectors such that the facial parts fit as closely as possible.\n",
    "\n",
    "Ref.: \n",
    "* [Ordinary Procrustes Analysis](https://en.wikipedia.org/wiki/Procrustes_analysis#Ordinary_Procrustes_analysis)\n",
    "* [Singular Value Decomposition](https://en.wikipedia.org/wiki/Singular_value_decomposition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def transformation_from_points(landmarks, target_landmarks):\n",
    "    \"\"\"\n",
    "    Transforms matrice p2 to fit into p1. Returns an affine\n",
    "    transformation [s * R | T] such that:\n",
    "        sum ||s*R*p1,i + T - p2,i||^2\n",
    "    is minimized.\n",
    "    \"\"\"\n",
    "    # extract align points and convert matrices into floats \n",
    "    p1 = target_landmarks[align_points].astype(np.float64)\n",
    "    p2 = landmarks[align_points].astype(np.float64)\n",
    "    # calculate centroids\n",
    "    c1 = np.mean(p1, axis=0)\n",
    "    c2 = np.mean(p2, axis=0)\n",
    "    # subtract centroid from each of the point sets\n",
    "    p1 -= c1\n",
    "    p2 -= c2\n",
    "    # find and use scaling\n",
    "    s1 = np.std(p1)\n",
    "    s2 = np.std(p2)\n",
    "    p1 /= s1\n",
    "    p2 /= s2\n",
    "    # calculate rotation using singular value decomposition\n",
    "    U, S, Vt = np.linalg.svd(p1.T * p2)\n",
    "    # The R we seek is in fact the transpose of the one given by U * Vt. This\n",
    "    # is because the above formulation assumes the matrix goes on the right\n",
    "    # (with row vectors) where as our solution requires the matrix to be on the\n",
    "    # left (with column vectors).\n",
    "    R = (U * Vt).T\n",
    "    # return affine transformation matrix\n",
    "    return np.vstack([\n",
    "        np.hstack((\n",
    "            (s2 / s1) * R,\n",
    "            c2.T - (s2 / s1) * R * c1.T\n",
    "        )),\n",
    "        np.matrix([0., 0., 1.])\n",
    "    ])\n",
    "\n",
    "def warp_image(img, target_img, M):\n",
    "    \"\"\"\n",
    "    use OpenCV's cv2.warpAffine to map the second image \n",
    "    onto the first.\n",
    "    \"\"\"\n",
    "    dshape = target_img.shape\n",
    "    result_img = np.zeros(dshape, dtype=img.dtype)\n",
    "    cv2.warpAffine(\n",
    "        img,\n",
    "        M[:2],\n",
    "        (dshape[1], dshape[0]),\n",
    "        dst=result_img,\n",
    "        borderMode=cv2.BORDER_TRANSPARENT,\n",
    "        flags=cv2.WARP_INVERSE_MAP\n",
    "    )\n",
    "    return result_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply transformation on sample2 image\n",
    "M = transformation_from_points(    \n",
    "    data[sample2]['landmarks'],\n",
    "    data[sample1]['landmarks']\n",
    ")\n",
    "warped_img = warp_image(\n",
    "    data[sample2]['raw_image'],\n",
    "    data[sample1]['raw_image'], \n",
    "    M\n",
    ")\n",
    "write_image('warped_{}'.format(sample2), warped_img)\n",
    "print('> generated warped image of \\'{}\\''.format(sample2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformed sample 2 image:\n",
    "<table><tr><td><img src=\"./Andrea_V.jpg\" alt=\"annotated sample 1\" style=\"height: 320px;\"/></td>\n",
    "<td><img src=\"./generated/warped_The_Equestrian_Session.jpg\" alt=\"annotated sample 2\" style=\"height: 320px;\"/></td></tr></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Blending face features together\n",
    "\n",
    "A mask is used to select which parts of image 2 and wich parts of image 1 should be shown in the final image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def draw_convex_hull(img, points, color):\n",
    "    \"\"\"\n",
    "    draw and fill a conves hull.\n",
    "    \"\"\"\n",
    "    points = cv2.convexHull(points)\n",
    "    cv2.fillConvexPoly(img, points, color=color)\n",
    "    \n",
    "def get_face_mask(img, landmarks):\n",
    "    \"\"\"\n",
    "    extracts face mask using the landmark matrix and draws two\n",
    "    convex polygons in white: one surrounding the eye area,\n",
    "    and one surrounding the nose and mouth area.\n",
    "    \n",
    "    Then it feathers the edge of the mask helping hide any\n",
    "    remaining discontinuities.\n",
    "    \"\"\"\n",
    "    img = np.zeros(img.shape[:2], dtype=np.float64)\n",
    "    for grp in overlay_points:\n",
    "        draw_convex_hull(\n",
    "            img,\n",
    "            landmarks[grp],\n",
    "            color=1\n",
    "        )\n",
    "    img = np.array([img, img, img]).transpose((1, 2, 0))\n",
    "    img = (cv2.GaussianBlur(img, (feather_amount, feather_amount), 0) > 0) * 1.0\n",
    "    img = cv2.GaussianBlur(img, (feather_amount, feather_amount), 0)\n",
    "    return img\n",
    "\n",
    "\n",
    "def get_combined_face_mask(img, landmarks, target_img, target_landmarks, M):\n",
    "    \"\"\"\n",
    "    gets combined face masks of the face parts\n",
    "    \"\"\"\n",
    "    # generate face masks for both images.\n",
    "    mask_img1 = get_face_mask(target_img, target_landmarks)\n",
    "    mask_img2 = get_face_mask(img, landmarks)\n",
    "    # transform second image into image 1's coordinate space\n",
    "    warped_mask_img2 = warp_image(mask_img2, target_img, M)\n",
    "    # combine masks by taking element-wise maximum, this guarantees\n",
    "    # that image 1 features are covered up and features of image 2\n",
    "    # can show through\n",
    "    combined_masks = np.max([mask_img1, warped_mask_img2], axis=0)\n",
    "    return combined_masks\n",
    "\n",
    "\n",
    "def apply_face_mask(img, landmarks, target_img, target_landmarks, M):\n",
    "    \"\"\"\n",
    "    extracts and combines the face parts\n",
    "    \"\"\"\n",
    "    combined_masks = get_combined_face_mask(img, landmarks, target_img, target_landmarks, M)\n",
    "    warped_img2 = warp_image(img, target_img, M)\n",
    "    # apply combined face mask to the target image\n",
    "    combined_img = target_img * (1.0 - combined_masks) + warped_img2 * combined_masks\n",
    "    return combined_img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform and merge the face parts\n",
    "combined_masks = get_combined_face_mask(\n",
    "    data[sample2]['raw_image'],\n",
    "    data[sample2]['landmarks'],\n",
    "    data[sample1]['raw_image'],\n",
    "    data[sample1]['landmarks'],\n",
    "    M\n",
    ")\n",
    "write_image(\n",
    "    'masked_{}'.format(sample1), \n",
    "    data[sample1]['raw_image'] * (1.0 - combined_masks)\n",
    ")\n",
    "result_img = apply_face_mask(\n",
    "    data[sample2]['raw_image'],\n",
    "    data[sample2]['landmarks'],\n",
    "    data[sample1]['raw_image'],\n",
    "    data[sample1]['landmarks'],\n",
    "    M\n",
    ")\n",
    "write_image('combined_{}'.format(sample1), result_img)\n",
    "print('> generated combined image of \\'{}\\' and \\'{}\\''.format(\n",
    "    sample1, \n",
    "    sample2\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merged face parts together:\n",
    "<table><tr><td><img src=\"./generated/masked_Andrea_V.jpg\" alt=\"masked sample 1\" style=\"height: 320px;\"/></td>\n",
    "<td><img src=\"./generated/combined_Andrea_V.jpg\" alt=\"merged face sample 1\" style=\"height: 320px;\"/></td></tr></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Color Correction\n",
    "\n",
    "In the last step we'll handle the color tones and brightness issues which cause discontinuities around the edges of the overlaid region.\n",
    "\n",
    "Ref.: [RGB scaling and color balance](https://en.wikipedia.org/wiki/Color_balance#Scaling_monitor_R.2C_G.2C_and_B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def correct_colors(img, target_img, target_landmarks):\n",
    "    \"\"\"\n",
    "    crude solution to change the image color to match the color\n",
    "    of the target image by using an appropriate size gaussian kernel.\n",
    "    \"\"\"\n",
    "    # kernel of 0.6 * pupillary distance\n",
    "    blur_amount = (\n",
    "        color_correction_blur_fraction * \n",
    "        np.linalg.norm(\n",
    "           np.mean(target_landmarks[left_eye_points], axis=0) - \n",
    "           np.mean(target_landmarks[right_eye_points], axis=0)\n",
    "        )\n",
    "    )\n",
    "    blur_amount = int(blur_amount)\n",
    "    if blur_amount%2 == 0:\n",
    "        blur_amount += 1\n",
    "    img1_blur = cv2.GaussianBlur(target_img, (blur_amount, blur_amount), 0)    \n",
    "    img2_blur = cv2.GaussianBlur(img, (blur_amount, blur_amount), 0)\n",
    "    img2_blur += (128 * (img2_blur <= 1.0)).astype(img2_blur.dtype)\n",
    "\n",
    "    return (\n",
    "        img.astype(np.float64) * \n",
    "        img1_blur.astype(np.float64) /\n",
    "        img2_blur.astype(np.float64)\n",
    "    )\n",
    "\n",
    "def swap_face(img, landmarks, target_img, target_landmarks, M):\n",
    "    \"\"\"\n",
    "    extracts and combines the face parts using color correction\n",
    "    \"\"\"\n",
    "    combined_mask = get_combined_face_mask(img, landmarks, target_img, target_landmarks, M)\n",
    "    warped_img = warp_image(img, target_img, M)\n",
    "    write_image('debug1_{}'.format(sample1), warped_img)\n",
    "    corrected_img = correct_colors(warped_img, target_img, target_landmarks)\n",
    "    write_image('debug2_{}'.format(sample1), corrected_img)\n",
    "    write_image('debug3_{}'.format(sample1), target_img)\n",
    "    write_image('debug4_{}'.format(sample1), corrected_img * combined_masks)\n",
    "    # apply combined face mask to the target image\n",
    "    combined_img = target_img * (1.0 - combined_mask) + corrected_img * combined_mask\n",
    "    return combined_img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the final swap face method\n",
    "result_img = swap_face(\n",
    "    data[sample2]['raw_image'],\n",
    "    data[sample2]['landmarks'],\n",
    "    data[sample1]['raw_image'],\n",
    "    data[sample1]['landmarks'],\n",
    "    M\n",
    ")\n",
    "write_image('final_{}'.format(sample1), result_img)\n",
    "print('> generated face swapped image of \\'{}\\' and \\'{}\\''.format(\n",
    "    sample1, \n",
    "    sample2\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final output:\n",
    "<table><tr><td><img src=\"./Andrea_V.jpg\" alt=\"original sample 1\" style=\"height: 320px;\"/></td><td><img src=\"./generated/debug2_Andrea_V.jpg\" alt=\"color corrected sample 2\" style=\"height: 320px;\"/></td>\n",
    "<td><img src=\"./generated/final_Andrea_V.jpg\" alt=\"final face sample 1\" style=\"height: 320px;\"/></td></tr></table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
